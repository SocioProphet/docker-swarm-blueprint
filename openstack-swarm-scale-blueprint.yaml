tosca_definitions_version: cloudify_dsl_1_3

imports:
  - http://www.getcloudify.org/spec/cloudify/4.0/types.yaml
  - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.5/plugin.yaml
  - imports/scale.yaml
  - imports/swarm-blueprint.yaml

inputs:

  image:
    description: image

  flavor:
    description: flavor

  agent_user:
    description: user for agents

  key_name:
    description: >
      the agent public key name (created during manager bootstrap)
    type: string
    default: docker-swarm-blueprint-key

  private_key_path:
    type: string
    default: ~/.ssh/docker-swarm-blueprint-key.pem

  external_network_name:
    default: external

  router_name:
    description: The Router Name

  public_network_name:
    description: The name of the Openstack public network.

  public_subnet_name:
    description: The name of the public network subnet.

  private_network_name:
    description: The name of the Openstack private network.

  private_subnet_name:
    description: The name of the private network subnet.

  region:
    default: ''

dsl_definitions:

  openstack_config: &openstack_config
    username: { get_secret: keystone_username }
    password: { get_secret: keystone_password }
    tenant_name: { get_secret: keystone_tenant_name }
    auth_url: { get_secret: keystone_url }
    region: { get_input: region }

node_templates:

  managers_tier:
    type: cloudify.nodes.Tier

  workers_tier:
    type: cloudify.nodes.Tier

  manager_host:
    type: cloudify.openstack.nodes.Server
    properties:
      openstack_config: *openstack_config
      agent_config:
        user: { get_input: agent_user }
        key: { get_property: [ key, private_key_path ] }
        install_method: remote
        port: 22
        min_workers: 2
      server:
        image: {get_input: image}
        flavor: {get_input: flavor}
        userdata: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
      management_network_name: { get_property: [ public_network, resource_id ] }
    relationships:
      - target: key
        type: cloudify.openstack.server_connected_to_keypair
      - target: manager_host_port
        type: cloudify.openstack.server_connected_to_port
      - type: cloudify.relationships.contained_in
        target: managers_tier

  manager_host_port:
    type: cloudify.openstack.nodes.Port
    properties:
      openstack_config: *openstack_config
    relationships:
      - type: cloudify.relationships.contained_in
        target: public_network
      - type: cloudify.relationships.depends_on
        target: public_subnet
      - type: cloudify.openstack.port_connected_to_security_group
        target: swarm_secgroup
      - type: cloudify.openstack.port_connected_to_floating_ip
        target: manager_public_ip

  worker_host:
    type: cloudify.openstack.nodes.Server
    properties:
      openstack_config: *openstack_config
      agent_config:
        user: { get_input: agent_user }
        key: { get_property: [ key, private_key_path ] }
        install_method: remote
        port: 22
        min_workers: 2
      server:
        image: {get_input: image}
        flavor: {get_input: flavor}
        userdata: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
      management_network_name: { get_property: [ private_network, resource_id ] }
    relationships:
      - target: key
        type: cloudify.openstack.server_connected_to_keypair
      - target: worker_host_port
        type: cloudify.openstack.server_connected_to_port
      - type: cloudify.relationships.contained_in
        target: managers_tier
    interfaces:
      cloudify.interfaces.monitoring_agent:
          install:
            implementation: diamond.diamond_agent.tasks.install
            inputs:
              diamond_config:
                interval: 2
          start: diamond.diamond_agent.tasks.start
          stop: diamond.diamond_agent.tasks.stop
          uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
          start:
            implementation: diamond.diamond_agent.tasks.add_collectors
            inputs:
              collectors_config:
                CPUCollector: {}
                MemoryCollector: {}
                LoadAverageCollector: {}
                DiskUsageCollector:
                  config:
                    devices: x?vd[a-z]+[0-9]*$
                NetworkCollector: {}

  worker_host_port:
    type: cloudify.openstack.nodes.Port
    properties:
      openstack_config: *openstack_config
    relationships:
      - type: cloudify.relationships.contained_in
        target: private_network
      - type: cloudify.relationships.depends_on
        target: private_subnet
      - type: cloudify.openstack.port_connected_to_security_group
        target: swarm_secgroup

  swarm_secgroup:
    type: cloudify.openstack.nodes.SecurityGroup
    properties:
      openstack_config: *openstack_config
      resource_id: swarm_secgroup
      rules:
        - remote_ip_prefix: 0.0.0.0/0
          port: 22
        - remote_ip_prefix: 0.0.0.0/0
          port: 2377
        - remote_ip_prefix: 0.0.0.0/0
          port: 80

  manager_public_ip:
    type: cloudify.openstack.nodes.FloatingIP
    properties:
      openstack_config: *openstack_config
      floatingip:
        floating_network_name: { get_property: [ external_network, resource_id ] }

  private_subnet:
    type: cloudify.openstack.nodes.Subnet
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: private_subnet_name }
    relationships:
      - target: private_network
        type: cloudify.relationships.contained_in

  private_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: private_network_name }

  public_subnet:
    type: cloudify.openstack.nodes.Subnet
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: public_subnet_name }
    relationships:
      - target: public_network
        type: cloudify.relationships.contained_in
      - target: router
        type: cloudify.openstack.subnet_connected_to_router

  public_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: public_network_name }

  router:
    type: cloudify.openstack.nodes.Router
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: router_name }
    relationships:
      - target: external_network
        type: cloudify.relationships.connected_to

  external_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: external_network_name }

  key:
    type: cloudify.openstack.nodes.KeyPair
    properties:
      openstack_config: *openstack_config
      resource_id: { get_input: key_name }
      private_key_path: { get_input: private_key_path }

groups:

 scale_up_group:
   members: [worker_host]
   policies:
     auto_scale_up:
       type: scale_policy_type
       properties:
         policy_operates_on_group: true
         scale_limit: 4
         scale_direction: '<'
         scale_threshold: 50
         service_selector: .*worker_host.*cpu.total.user
         cooldown_time: 120
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: 1
               scalable_entity_name: worker_host
               scale_compute: true

 scale_down_group:
   members: [worker_host]
   policies:
     auto_scale_down:
       type: scale_policy_type
       properties:
         scale_limit: 1
         scale_direction: '>'
         scale_threshold: 10
         service_selector: .*worker_host.*cpu.total.user
         cooldown_time: 60
         moving_window_size: 60
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: -1
               scalable_entity_name: worker_host
               scale_compute: true

 heal_group:
   members: [worker_host]
   policies:
     simple_autoheal_policy:
       type: cloudify.policies.types.host_failure
       properties:
         service:
           - .*worker_host.*cpu.total.system
           - .*manager_host.*cpu.total.system
         interval_between_workflows: 60
       triggers:
         auto_heal_trigger:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: heal
             workflow_parameters:
               node_instance_id: { 'get_property': [ SELF, node_id ] }
               diagnose_value: { 'get_property': [ SELF, diagnose ] }

outputs:
  manager_ip:
    description: swarm manager ip
    value: {get_attribute: [manager_public_ip, floating_ip_address]}
